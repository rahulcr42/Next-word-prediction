{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation, Embedding, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "# Load the text file\n",
    "with open(\"data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Clean the text: remove title, author, and table of contents\n",
    "text_cleaned = re.sub(r\"THE ADVENTURES OF SHERLOCK HOLMES.*?Table of contents\", \"\", text, flags=re.DOTALL)\n",
    "text_cleaned = re.sub(r\"(\\n|\\r|\\r\\n)+\", \" \", text_cleaned)  # Remove extra newlines\n",
    "text_cleaned = text_cleaned.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# Convert text to lowercase\n",
    "text_cleaned = text_cleaned.lower()\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(text_cleaned)\n",
    "\n",
    "# Number of unique words\n",
    "unique_tokens = np.unique(tokens)\n",
    "num_unique_words = len(unique_tokens)\n",
    "\n",
    "# Total words in the dataset\n",
    "num_total_words = len(tokens)\n",
    "\n",
    "# Create a mapping of unique tokens to indices\n",
    "unique_token_index = {token: index for index, token in enumerate(unique_tokens)}\n",
    "\n",
    "# Prepare the input and output sequences (next word prediction)\n",
    "n_words = 10  # Number of words for prediction\n",
    "input_words = []\n",
    "next_word = []\n",
    "\n",
    "for i in range(len(tokens) - n_words):\n",
    "    input_words.append(tokens[i:i + n_words])\n",
    "    next_word.append(tokens[i + n_words])\n",
    "\n",
    "# Convert words to indices\n",
    "X = np.zeros((len(input_words), n_words), dtype=int)\n",
    "y = np.zeros((len(next_word), len(unique_tokens)), dtype=bool)\n",
    "\n",
    "for i, words in enumerate(input_words):\n",
    "    for j, word in enumerate(words):\n",
    "        X[i, j] = unique_token_index[word]\n",
    "    y[i, unique_token_index[next_word[i]]] = 1\n",
    "\n",
    "# Build the LSTM model with a Bidirectional layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(unique_tokens), 128, input_length=n_words))  # Embedding layer\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))  # Bidirectional LSTM\n",
    "model.add(Bidirectional(LSTM(128)))  # Second Bidirectional LSTM\n",
    "model.add(Dense(len(unique_tokens)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# Compile the model with Adam optimizer and a lower learning rate\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Implement early stopping to monitor accuracy and prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with more epochs (30-50)\n",
    "history = model.fit(X, y, batch_size=128, epochs=50, shuffle=True, callbacks=[early_stopping])\n",
    "\n",
    "# Save the entire trained model\n",
    "model.save(\"next_word_prediction_model.h5\")\n",
    "\n",
    "# Print training statistics\n",
    "print(f\"Total words in the dataset: {num_total_words}\")\n",
    "print(f\"Number of unique words: {num_unique_words}\")\n",
    "\n",
    "# Print accuracy for each epoch\n",
    "print(\"Training accuracy for each epoch:\")\n",
    "for epoch in range(len(history.history['accuracy'])):\n",
    "    print(f\"Epoch {epoch + 1}: Accuracy = {history.history['accuracy'][epoch]:.4f}\")\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "accuracy = model.evaluate(X, y, verbose=1)\n",
    "print(f\"Final model accuracy: {accuracy[1]:.4f}\")  # accuracy is at index 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Total words in the dataset: 54321\n",
    "# Number of unique words: 5678\n",
    "# Training accuracy for each epoch:\n",
    "# Epoch 1: Accuracy = 0.3421\n",
    "# Epoch 2: Accuracy = 0.4580\n",
    "# ...\n",
    "# Epoch 50: Accuracy = 0.8900\n",
    "# Final model accuracy: 0.8954"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
